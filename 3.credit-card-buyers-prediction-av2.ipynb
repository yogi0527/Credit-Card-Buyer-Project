{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing Library","metadata":{}},{"cell_type":"code","source":"try:\n    import pandas as pd\n    import numpy as np \n        \n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    sns.set(color_codes=True)\n    \n    print(\"all loaded\")\nexcept:\n    print(\"error\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading Dataset","metadata":{}},{"cell_type":"code","source":"# Loading data from train.csv file\ntrain_df = pd.read_csv(\"train.csv\")\ntrain_df.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading data from test.csv file\ntest_df = pd.read_csv(\"test.csv\")\ntest_df.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\ntest_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DATA PROCESSING","metadata":{}},{"cell_type":"markdown","source":"### STEP 1: Dealing with Null values","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.fillna(\"None\")\ntest_df = test_df.fillna(\"None\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2 : Dealing with Duplicated Values","metadata":{}},{"cell_type":"markdown","source":"#### Duplicate Rows:","metadata":{}},{"cell_type":"code","source":"train_df.duplicated().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Duplicate Columns:","metadata":{}},{"cell_type":"code","source":"# train_t = train_df.T\n# train_t.shape\n# print(train_t.duplicated().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No duplicate values thus are present","metadata":{}},{"cell_type":"markdown","source":"### Step 3: Handling Outliers","metadata":{}},{"cell_type":"markdown","source":"#### a) Finding Oultiers Using Boxplot:  Boxplot Outliers only for Numerical Variables and not categorical","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=train_df)\nplt.show","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### b) Finding Outliers Using Scatterplot","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=train_df[\"Avg_Account_Balance\"])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop(\"ID\",axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining Target Variable","metadata":{}},{"cell_type":"code","source":"train = train_df.drop([\"Is_Lead\"],axis=1)\ny = train_df[\"Is_Lead\"]\ntest = test_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Differentiating Numericala and Categorical Data for further processing","metadata":{}},{"cell_type":"code","source":"#We have 2 types of data in our dataset : int64 and object\n\ntrain_categorical = train.select_dtypes(exclude = ['int64'])\ntest_categorical = test.select_dtypes(exclude = ['int64'])\n\ntrain_numerical = train.select_dtypes(include = ['int64'])\ntest_numerical = test.select_dtypes(include = ['int64'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining column names for numerical data","metadata":{}},{"cell_type":"code","source":"numcol_names_train = train_numerical.columns.values\nnumcol_names_test = test_numerical.columns.values\n\nnumcol_names_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Converting these to list from array\n\nnumcol_names_train.tolist()\nnumcol_names_test.tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking Skewness and Kurtosis for Numerical Columns","metadata":{}},{"cell_type":"markdown","source":"#### For Train Data","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(train_numerical['Avg_Account_Balance'], bw=0.5)    #bw is smoothing parameter\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(train_numerical['Age'], bw=0.5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(train_numerical['Vintage'],bw=0.5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### For test Data ","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(test_numerical['Avg_Account_Balance'], bw=0.5)    #bw is smoothing parameter\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is skewness in Avg Account Balance Column for bith train and test data.","metadata":{}},{"cell_type":"markdown","source":"#### Dealing with Skewness Using Log Transformation","metadata":{}},{"cell_type":"code","source":"train_numerical['Avg_Account_Balance'] = np.log(train_numerical['Avg_Account_Balance'])\nsns.kdeplot(train_numerical['Avg_Account_Balance'])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_numerical['Avg_Account_Balance'] = np.log(test_numerical['Avg_Account_Balance'])\nsns.kdeplot(test_numerical['Avg_Account_Balance'])\nplt.show","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_numerical.agg(['skew', 'kurtosis'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Both skew and kurtosis can be analyzed through descriptive statistics. Acceptable values of skewness fall between − 3 and + 3, and kurtosis is appropriate from a range of − 10 to + 10","metadata":{}},{"cell_type":"code","source":"#Histograms for numerical Columns\ntrain_numerical.hist(figsize=(15, 10), bins=50, xlabelsize=8, ylabelsize=8);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalinzing and Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#Using standard scaler\nscaler = StandardScaler()\ntrain_numerical = scaler.fit_transform(train_numerical.values)\ntrain_numerical = pd.DataFrame(train_numerical, columns = numcol_names_train)\n\ntest_numerical = scaler.fit_transform(test_numerical.values)\ntest_numerical = pd.DataFrame(test_numerical, columns = numcol_names_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding Categorical Data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ntrain_categorical = train_categorical.apply(LabelEncoder().fit_transform)\ntest_categorical = test_categorical.apply(LabelEncoder().fit_transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(train_categorical)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Combining the Numnerical and Categorical Database","metadata":{}},{"cell_type":"code","source":"train_new = pd.concat([train_categorical,train_numerical,y],axis=1)\ntest_new = pd.concat([test_categorical,test_numerical],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking For Correlations:","metadata":{}},{"cell_type":"code","source":"#For coplete Database\n\ncorr_train = train_new.corr()\nplt.figure(figsize=(13,5)) \n\n\nax = sns.heatmap(corr_train,annot=True)\nplt.show\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Variable Correlation with Target Variable:","metadata":{}},{"cell_type":"code","source":"imp = train_new.drop(\"Is_Lead\", axis=1).apply(lambda x: x.corr(train_new.Is_Lead))\nprint(imp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = np.argsort(imp)\nprint(imp[indices])     #Sorted in ascending order","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing Variable with Low correlation with Target Variables","metadata":{}},{"cell_type":"code","source":"for i in range(0, len(indices)):\n    if np.abs(imp[i])>0.02:\n        print(train_new.columns[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can drop occupation and Id","metadata":{}},{"cell_type":"code","source":"# train_new1 = train_new.drop([\"Occupation\"],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# names=['cylinders','displacement','horsepower','weight','acceleration','model year', 'name']\n# plt.title('Miles Per Gallon')\n\n# #Plotting horizontal bar graph\n# plt.barh(range(len(indices)), imp[indices], color='g', align='center')\n# plt.yticks(range(len(indices)), [names[i] for i in indices])\n# plt.xlabel('Relative Importance')\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Predictors Co-relation With each other","metadata":{}},{"cell_type":"code","source":"for i in range(0,len(train_new1.columns)):\n    for j in  range(0,len(train_new1.columns)):\n        if i!=j:\n            corr_1=np.abs(train_new1[train_new1.columns[i]].corr(train_new1[train_new1.columns[j]]))\n            if corr_1 <0.3:\n                print( train_new1.columns[i] , \" is not correlated  with \", train_new1.columns[j])\n            elif corr_1>0.75:\n                print( train_new1.columns[i] , \" is highly  correlated  with \", train_new1.columns[j])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus there exist no such great correlation mong our variables. Great news!","metadata":{}},{"cell_type":"markdown","source":"### Find Mutual Information OR Information Gain","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\n\ncol = train_new.drop([\"Is_Lead\"],axis=1)\n       \nmig = mutual_info_regression(col, y);\nmig","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mig = pd.Series(mig)\nmig.index = col.columns\nmig","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the mutual information\n\nmig.sort_values(ascending=False).plot.bar(figsize=(10, 4))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelling","metadata":{}},{"cell_type":"code","source":"# Split the Train data into predictors and target\n\nX = train_new.drop(['Is_Lead'],axis=1)\npredictor_test = test_new.drop(['ID'], axis =1)\ny = train_new['Is_Lead']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_test.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Evaluation Metric & Cross Validation Libraries\nfrom sklearn.metrics import *\n# Boosting Algorithm Librarie\nimport xgboost\nfrom lightgbm import LGBMClassifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LGBMClassifier(metric = 'auc', \n                       n_estimators=50000,    \n                       bagging_fraction=0.95, \n                       subsample_freq = 2, \n                       objective =\"binary\",\n                       importance_type = \"gain\",\n                       verbosity = -1,\n                       random_state=294,\n                       num_leaves = 300,\n                       boosting_type = 'gbdt',\n                       learning_rate=0.15,\n                       max_depth=4, \n                       scale_pos_weight=2,\n                       n_jobs=-1 \n                      )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\naccuracy = []\nskf = StratifiedKFold(n_splits=10,shuffle=True)\n\nskf.get_n_splits(X,y)   #this will return train_index and test_index\n\nfor train_index,test_index in skf.split(X,y):\n    print(\"Train\",train_index,\"Validation:\",test_index)\n    print(train_index.shape,test_index.shape)\n    \n    X_train,X_test = X.iloc[train_index],X.iloc[test_index]\n    y_train,y_test = y.iloc[train_index],y.iloc[test_index]\n    \n    model.fit(X_train,y_train,eval_set=[(X_train, y_train),(X_test, y_test)],early_stopping_rounds=100 ,verbose=100)\n    pred = model.predict(X_test)\n    \n    score = accuracy_score(pred,y_test)\n    \n    accuracy.append(score)\n    print(np.array(accuracy).mean())\n    \n    \n    rocauc = []\n    \n\n    roc_auc = roc_auc_score(y_test,model.predict_proba(X_test)[:, 1])\n    rocauc.append(roc_auc)\n   \n    print(np.array(roc_auc).mean())","metadata":{},"execution_count":null,"outputs":[]}]}